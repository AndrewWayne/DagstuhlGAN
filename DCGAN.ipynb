{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/jacobgil/keras-dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Reshape, Cropping2D, ZeroPadding2D, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_DIMS = 16\n",
    "MAP_TILES = 11\n",
    "MAP_WIDTH = 28\n",
    "MAP_HEIGHT = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_shape=(Z_DIMS,), activation='tanh'))\n",
    "    model.add(Reshape((1,2,128)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(32,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(16,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(8,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(Dense(MAP_TILES,activation='softmax'))\n",
    "    model.add(Cropping2D(((0,16-MAP_HEIGHT),(0,32-MAP_WIDTH))))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D(((0,16-MAP_HEIGHT),(0,32-MAP_WIDTH)),input_shape=(MAP_HEIGHT,MAP_WIDTH,MAP_TILES)))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Conv2D(16,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Reshape((256,)))\n",
    "    model.add(Dense(Z_DIMS,activation='tanh'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tiles2image(tiles):\n",
    "    return get_cmap('rainbow')(tiles/MAP_TILES)\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1],shape[2]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"examples.json\") as f:\n",
    "    X_train = array(json.load(f))    \n",
    "\n",
    "figsize(20,20)\n",
    "    \n",
    "imshow(tiles2image(X_train[200]))\n",
    "figure()\n",
    "random.shuffle(X_train)\n",
    "imshow(combine_images(tiles2image(X_train)))\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE, X_train):\n",
    "    \n",
    "    d = make_discriminator_model()\n",
    "    g = make_generator_model()\n",
    "    \n",
    "    d_on_g = make_generator_containing_discriminator(g, d)\n",
    "    \n",
    "    # original learning rate in example code was 0.0005\n",
    "    d_optim = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    g.compile(loss='categorical_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    with open(\"generator.json\",\"w\") as f:\n",
    "        f.write(g.to_json())\n",
    "        \n",
    "    with open(\"discriminator.json\",\"w\") as f:\n",
    "        f.write(d.to_json())\n",
    "    \n",
    "    fake_archive = []\n",
    "    reference_noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, Z_DIMS))\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        #print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        \n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            \n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, Z_DIMS))\n",
    "            true_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            fake_batch = g.predict(noise, verbose=0).argmax(-1)\n",
    "            \n",
    "            # to stabilize training: train on a random fake batch from the past\n",
    "            fake_archive.append(fake_batch)\n",
    "            fake_batch = random.choice(fake_archive)\n",
    "        \n",
    "            X = np.concatenate((true_batch, fake_batch))\n",
    "            y = [1] * len(true_batch) + [0] * len(fake_batch)\n",
    "            \n",
    "            X_onehot = to_categorical(X.ravel(), num_classes=MAP_TILES)\\\n",
    "                .reshape((len(X),MAP_HEIGHT,MAP_WIDTH,MAP_TILES))\n",
    "        \n",
    "            d.trainable = True\n",
    "            d_loss = d.train_on_batch(X_onehot, y)\n",
    "            \n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, Z_DIMS))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            \n",
    "            print(\".\",end='')\n",
    "          \n",
    "        print(\"\")\n",
    "        \n",
    "        sample_batch = g.predict(reference_noise, verbose=0).argmax(-1)\n",
    "        image = combine_images(tiles2image(sample_batch))\n",
    "        imsave(\"sample_\"+str(epoch)+\".png\", image)\n",
    "        with open(\"sample_\"+str(epoch)+\".json\",\"w\") as f:\n",
    "            json.dump(sample_batch.tolist(),f)\n",
    "        \n",
    "        g.save_weights('generator.h5', True)\n",
    "        d.save_weights('discriminator.h5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm sample*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "train(batch_size, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
