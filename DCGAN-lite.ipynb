{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/jacobgil/keras-dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Reshape, Cropping2D, ZeroPadding2D\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_DIMS = 16\n",
    "MAP_TILES = 11\n",
    "MAP_WIDTH = 28\n",
    "MAP_HEIGHT = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_shape=(Z_DIMS,), activation='tanh'))\n",
    "    model.add(Reshape((4,8,4)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(4,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(4,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(Dense(MAP_TILES,activation='softmax'))\n",
    "    model.add(Cropping2D(((0,16-MAP_HEIGHT),(0,32-MAP_WIDTH))))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_generator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D(((0,16-MAP_HEIGHT),(0,32-MAP_WIDTH)),input_shape=(MAP_HEIGHT,MAP_WIDTH,MAP_TILES)))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Conv2D(4,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Conv2D(4,(3,3),padding='same',activation='tanh'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Reshape((128,)))\n",
    "    model.add(Dense(Z_DIMS,activation='tanh'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_discriminator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tiles2image(tiles):\n",
    "    return get_cmap('rainbow')(tiles/MAP_TILES)\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1],shape[2]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"examples.json\") as f:\n",
    "    X_train = array(json.load(f))    \n",
    "\n",
    "figsize(20,20)\n",
    "    \n",
    "imshow(tiles2image(X_train[200]))\n",
    "figure()\n",
    "random.shuffle(X_train)\n",
    "imshow(combine_images(tiles2image(X_train)))\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE, X_train):\n",
    "    \n",
    "    d = make_discriminator_model()\n",
    "    g = make_generator_model()\n",
    "    \n",
    "    d_on_g = make_generator_containing_discriminator(g, d)\n",
    "    \n",
    "    d_optim = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    g.compile(loss='categorical_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    with open(\"generator.json\",\"w\") as f:\n",
    "        f.write(g.to_json())\n",
    "        \n",
    "    with open(\"discriminator.json\",\"w\") as f:\n",
    "        f.write(d.to_json())\n",
    "    \n",
    "    reference_noise = np.random.multivariate_normal(zeros(Z_DIMS),eye(Z_DIMS),size=(BATCH_SIZE,))\n",
    "    reference_select = np.random.rand(len(reference_noise),MAP_HEIGHT,MAP_WIDTH,1)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        #print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        \n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            \n",
    "            for i in range(10):\n",
    "                noise = np.random.multivariate_normal(zeros(Z_DIMS),eye(Z_DIMS),size=(BATCH_SIZE,))\n",
    "\n",
    "                true_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "\n",
    "                fake_batch_dist = g.predict(noise, verbose=0)\n",
    "                fake_batch_select = np.random.rand(BATCH_SIZE,MAP_HEIGHT,MAP_WIDTH,1)\n",
    "                fake_batch = (fake_batch_select < fake_batch_dist.cumsum(axis=3)).argmax(-1)\n",
    "\n",
    "                X_hard = np.concatenate((true_batch, fake_batch))\n",
    "                y = [1] * len(true_batch) + [0] * len(fake_batch)\n",
    "\n",
    "                X = to_categorical(X_hard.ravel(), num_classes=MAP_TILES)\\\n",
    "                    .reshape((len(X_hard),MAP_HEIGHT,MAP_WIDTH,MAP_TILES))\n",
    "                \n",
    "                d_loss = d.train_on_batch(X, y)\n",
    "            \n",
    "            noise = np.random.multivariate_normal(zeros(Z_DIMS),eye(Z_DIMS),size=(BATCH_SIZE,))\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            \n",
    "            print(\".\",end='')\n",
    "          \n",
    "        print(\"\")\n",
    "        \n",
    "        sample_batch = g.predict(reference_noise, verbose=0)\n",
    "        image = combine_images(tiles2image(sample_batch.argmax(-1)))\n",
    "        imsave(\"sample_\"+str(epoch)+\".png\", image)\n",
    "        image = combine_images(sample_batch[:,:,:,2].reshape(BATCH_SIZE,MAP_HEIGHT,MAP_WIDTH,1).repeat(3,axis=3))\n",
    "        imsave(\"sample_\"+str(epoch)+\".weight.png\", image)\n",
    "        sample_pick = (reference_select < sample_batch.cumsum(axis=3)).argmax(-1)\n",
    "        image = combine_images(tiles2image(sample_pick))\n",
    "        imsave(\"sample_\"+str(epoch)+\".pick.png\", image)\n",
    "        \n",
    "        #with open(\"sample_\"+str(epoch)+\".json\",\"w\") as f:\n",
    "        #    json.dump(sample_batch.tolist(),f)\n",
    "        \n",
    "        g.save_weights('generator.h5', True)\n",
    "        d.save_weights('discriminator.h5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm sample*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "train(batch_size, X_train==2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
